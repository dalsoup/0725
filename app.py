import streamlit as st
import pandas as pd
import datetime
import requests
import os
import base64
from urllib.parse import unquote

from utils import (
    get_weather, get_asos_weather, get_risk_level,
    calculate_avg_temp, region_to_stn_id
)
from model_utils import predict_from_weather

# ----------------------- ğŸ“¦ ì„¤ì • -----------------------
st.set_page_config(layout="centered")

KMA_API_KEY = unquote(st.secrets["KMA"]["API_KEY"])
ASOS_API_KEY = unquote(st.secrets["ASOS"]["API_KEY"])
GITHUB_USERNAME = st.secrets["GITHUB"]["USERNAME"]
GITHUB_REPO = st.secrets["GITHUB"]["REPO"]
GITHUB_BRANCH = st.secrets["GITHUB"]["BRANCH"]
GITHUB_TOKEN = st.secrets["GITHUB"]["TOKEN"]
GITHUB_FILENAME = "ML_asos_dataset.csv"

# ----------------------- ğŸ§­ UI ì‹œì‘ -----------------------
st.title("HeatAI")
tab1, tab2, tab3 = st.tabs(["ğŸ“Š í­ì—¼ í”¼í•´ AI ì˜ˆì¸¡ íƒ­", "ğŸ“¥ ì‹¤ì œ í­ì—¼ í”¼í•´ ê¸°ë¡ íƒ­", "ğŸ“ ìì¹˜êµ¬ë³„ í”¼í•´ì ìˆ˜ ë° ë³´ìƒê¸ˆ ê³„ì‚°íƒ­"])

# ====================================================================
# ğŸ”® ì˜ˆì¸¡ íƒ­
# ====================================================================
with tab1:
    st.header("ğŸ“Š í­ì—¼ í”¼í•´ AI ì˜ˆì¸¡")

    # âœ… ë‚ ì§œ ì„ íƒ ë²”ìœ„ ì„¤ì •: 2021-05-01 ~ ì˜¤ëŠ˜+30ì¼
    today = datetime.date.today()
    min_pred_date = datetime.date(2021, 5, 1)
    max_pred_date = today + datetime.timedelta(days=30)

    # âœ… ì§€ì—­ ë° ë‚ ì§œ ì„ íƒ
    region = st.selectbox("ì§€ì—­ ì„ íƒ", list(region_to_stn_id.keys()), key="region_tab1")
    date_selected = st.date_input(
        "ë‚ ì§œ ì„ íƒ",
        value=today,
        min_value=min_pred_date,
        max_value=max_pred_date,
        key="date_tab1"
    )

    if st.button("ğŸ” ì˜ˆì¸¡í•˜ê¸°", key="predict_tab1"):
        # âœ… ê¸°ìƒ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
        if date_selected >= today:
            weather, base_date, base_time = get_weather(region, date_selected, KMA_API_KEY)
        else:
            ymd = date_selected.strftime("%Y%m%d")
            weather = get_asos_weather(region, ymd, ASOS_API_KEY)

        # âœ… ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê¸°ìƒ ì •ë³´ê°€ ì—†ì„ ê²½ìš° ì¤‘ë‹¨
        if not weather:
            st.error("âŒ ê¸°ìƒ ì •ë³´ ì—†ìŒ")
            st.stop()

        # âœ… ê¸°ì˜¨/ìŠµë„ ì¶”ì¶œ
        tmx = weather.get("TMX", 0)
        tmn = weather.get("TMN", 0)
        reh = weather.get("REH", 0)

        # âœ… ì˜ˆì¸¡ ìˆ˜í–‰
        pred, avg_temp, input_df = predict_from_weather(tmx, tmn, reh)
        risk = get_risk_level(pred)

        # âœ… ê¸°ìƒ ì •ë³´ ì¶œë ¥
        st.markdown("#### â˜ï¸ ê¸°ìƒì •ë³´")
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("ìµœê³ ê¸°ì˜¨", f"{tmx:.1f}â„ƒ")
        col2.metric("ìµœì €ê¸°ì˜¨", f"{tmn:.1f}â„ƒ")
        col3.metric("í‰ê· ê¸°ì˜¨", f"{avg_temp:.1f}â„ƒ")
        col4.metric("ìŠµë„", f"{reh:.1f}%")

        with st.expander("ğŸ§ª ì…ë ¥ê°’ í™•ì¸"):
            st.dataframe(input_df)

        # âœ… ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        st.markdown("#### ğŸ’¡ ì˜ˆì¸¡ ê²°ê³¼")
        c1, c2 = st.columns(2)
        c1.metric("ì˜ˆì¸¡ í™˜ì ìˆ˜", f"{pred:.2f}ëª…")
        c2.metric("ìœ„í—˜ ë“±ê¸‰", risk)

        # âœ… ì „ë…„ë„ í™˜ììˆ˜ ë¹„êµ
        def get_last_year_patient_count(current_date, region, static_file="ML_7_8ì›”_2021_2025_dataset.xlsx"):
            try:
                last_year_date = current_date - datetime.timedelta(days=365)
                df_all = pd.read_excel(static_file, engine="openpyxl")
                df_all["ì¼ì‹œ"] = pd.to_datetime("1899-12-30") + pd.to_timedelta(df_all["ì¼ì‹œ"], unit="D")
                df_all["ì¼ì"] = df_all["ì¼ì‹œ"].dt.strftime("%Y-%m-%d")
                cond = (df_all["ì¼ì"] == last_year_date.strftime("%Y-%m-%d")) & (df_all["ê´‘ì—­ìì¹˜ë‹¨ì²´"] == region)
                row = df_all[cond]
                if not row.empty:
                    return int(row["í™˜ììˆ˜"].values[0])
                else:
                    return None
            except:
                return None

        last_year_count = get_last_year_patient_count(date_selected, region)
        if last_year_count is not None:
            delta = pred - last_year_count
            st.markdown(f"ğŸ“… **ì „ë…„ë„({(date_selected - datetime.timedelta(days=365)).strftime('%Y-%m-%d')}) ë™ì¼ ë‚ ì§œ í™˜ììˆ˜**: **{last_year_count}ëª…**")
            st.markdown(f"ğŸ“ˆ **ì „ë…„ ëŒ€ë¹„ ì¦ê°**: {'+' if delta >= 0 else ''}{delta:.1f}ëª…")
        else:
            st.markdown("ğŸ“­ ì „ë…„ë„ ë™ì¼ ë‚ ì§œì˜ í™˜ì ìˆ˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # âœ… ì˜ˆì¸¡ê°’ CSVë¡œ ì €ì¥ (tab3ì—ì„œ í™œìš©)
        SAVE_FILE = "ML_asos_total_prediction.csv"
        try:
            df_total = pd.read_csv(SAVE_FILE, encoding="utf-8-sig")
        except FileNotFoundError:
            df_total = pd.DataFrame(columns=["ì¼ì", "ì„œìš¸ì‹œì˜ˆì¸¡í™˜ììˆ˜"])

        new_row = pd.DataFrame([{
            "ì¼ì": date_selected.strftime("%Y-%m-%d"),
            "ì„œìš¸ì‹œì˜ˆì¸¡í™˜ììˆ˜": round(pred, 2)
        }])

        df_total = df_total[df_total["ì¼ì"] != new_row.iloc[0]["ì¼ì"]]
        df_total = pd.concat([df_total, new_row], ignore_index=True)
        df_total.to_csv(SAVE_FILE, index=False, encoding="utf-8-sig")

        st.success(f"âœ… ì˜ˆì¸¡ê°’ì´ '{SAVE_FILE}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ====================================================================
# ğŸ“¥ ì‹¤ì œ í­ì—¼ í”¼í•´ ê¸°ë¡ íƒ­
# ====================================================================
with tab2:
    with st.expander("â„¹ï¸ tab2 ì‚¬ìš©ë²•"):
        st.markdown("""
        **âœ… ì‚¬ìš© ë°©ë²•**  
        1. ë‚ ì§œ(ë³µìˆ˜ ê°€ëŠ¥)ì™€ ìì¹˜êµ¬(ì „ì²´ ë˜ëŠ” ì¼ë¶€)ë¥¼ ì„ íƒí•˜ì„¸ìš”.  
        2. ì•„ë˜ ë§í¬ì—ì„œ ì§ˆë³‘ì²­ì˜ ì˜¨ì—´ì§ˆí™˜ì ì—‘ì…€ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•´ ì—…ë¡œë“œí•˜ì„¸ìš”.  
           ğŸ‘‰ [ì˜¨ì—´ì§ˆí™˜ ì‘ê¸‰ì‹¤ê°ì‹œì²´ê³„ ë‹¤ìš´ë¡œë“œ](https://www.kdca.go.kr/board/board.es?mid=a20205030102&bid=0004&&cg_code=C01)  
        3. **ì €ì¥í•˜ê¸°** ë²„íŠ¼ì„ ëˆ„ë¥´ë©´, í•´ë‹¹ ë°ì´í„°ëŠ” tab3ì˜ ìì¹˜êµ¬ë³„ í”¼í•´ì ìˆ˜ ì‚°ì •ì„ ìœ„í•œ ì…ë ¥ê°’ìœ¼ë¡œ ìë™ ë°˜ì˜ë©ë‹ˆë‹¤.
        """)

    st.header("ğŸ“¥ ì‹¤ì œ í­ì—¼ í”¼í•´ ê¸°ë¡í•˜ê¸°")

    region = st.selectbox("ğŸŒ ê´‘ì—­ì‹œë„ ì„ íƒ", ["ì„œìš¸íŠ¹ë³„ì‹œ"], key="region_tab2")

    all_gus = [
        'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ê´‘ì§„êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ì¤‘ë‘êµ¬', 'ì„±ë¶êµ¬', 'ê°•ë¶êµ¬', 'ë„ë´‰êµ¬',
        'ë…¸ì›êµ¬', 'ì€í‰êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ë§ˆí¬êµ¬', 'ì–‘ì²œêµ¬', 'ê°•ì„œêµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ì˜ë“±í¬êµ¬',
        'ë™ì‘êµ¬', 'ê´€ì•…êµ¬', 'ì„œì´ˆêµ¬', 'ê°•ë‚¨êµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬'
    ]

    gus = st.multiselect("ğŸ˜ï¸ ìì¹˜êµ¬ ì„ íƒ (ì„ íƒí•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´)", all_gus, key="gu_tab2_multi")
    if not gus:
        gus = all_gus

    # âœ… ë‚ ì§œ ì„ íƒ: pandas â†’ date ë³€í™˜
    min_record_date = datetime.date(2021, 5, 1)
    max_record_date = datetime.date.today() - datetime.timedelta(days=1)
    date_range = [d.date() for d in pd.date_range(min_record_date, max_record_date, freq='D')]

    dates_selected = st.multiselect("ğŸ“… ê¸°ë¡í•  ë‚ ì§œ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)", date_range, default=[max_record_date])

    uploaded_file = st.file_uploader("ğŸ“ ì§ˆë³‘ì²­ í™˜ììˆ˜ íŒŒì¼ ì—…ë¡œë“œ (.xlsx, ì‹œíŠ¸ëª…: ì„œìš¸íŠ¹ë³„ì‹œ)", type=["xlsx"], key="upload_tab2")

    if uploaded_file and dates_selected:
        try:
            df_raw = pd.read_excel(uploaded_file, sheet_name="ì„œìš¸íŠ¹ë³„ì‹œ", header=None)
            districts = df_raw.iloc[0, 1::2].tolist()
            dates = df_raw.iloc[3:, 0].tolist()
            df_values = df_raw.iloc[3:, 1::2]
            df_values.columns = districts
            df_values.insert(0, "ì¼ì", dates)
            df_long = df_values.melt(id_vars=["ì¼ì"], var_name="ìì¹˜êµ¬", value_name="í™˜ììˆ˜")
            df_long["ì¼ì"] = pd.to_datetime(df_long["ì¼ì"], errors="coerce").dt.strftime("%Y-%m-%d")
            df_long["í™˜ììˆ˜"] = pd.to_numeric(df_long["í™˜ììˆ˜"], errors="coerce").fillna(0).astype(int)
            df_long["ì§€ì—­"] = "ì„œìš¸íŠ¹ë³„ì‹œ"

            preview_list = []

            for date_selected in dates_selected:
                ymd = date_selected.strftime("%Y-%m-%d")

                weather = get_asos_weather(region, ymd.replace("-", ""), ASOS_API_KEY)
                tmx = weather.get("TMX", 0)
                tmn = weather.get("TMN", 0)
                reh = weather.get("REH", 0)
                avg_temp = calculate_avg_temp(tmx, tmn)

                for gu in gus:
                    selected = df_long[(df_long["ì¼ì"] == ymd) & (df_long["ìì¹˜êµ¬"] == gu)]
                    if selected.empty:
                        st.warning(f"âŒ {ymd} {gu} í™˜ììˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        continue

                    í™˜ììˆ˜ = int(selected["í™˜ììˆ˜"].values[0])
                    preview_list.append({
                        "ì¼ì": ymd,
                        "ì§€ì—­": region,
                        "ìì¹˜êµ¬": gu,
                        "ìµœê³ ì²´ê°ì˜¨ë„(Â°C)": tmx + 1.5,
                        "ìµœê³ ê¸°ì˜¨(Â°C)": tmx,
                        "í‰ê· ê¸°ì˜¨(Â°C)": avg_temp,
                        "ìµœì €ê¸°ì˜¨(Â°C)": tmn,
                        "í‰ê· ìƒëŒ€ìŠµë„(%)": reh,
                        "í™˜ììˆ˜": í™˜ììˆ˜
                    })

            if not preview_list:
                st.warning("âŒ ì„ íƒí•œ ë‚ ì§œì™€ ìì¹˜êµ¬ ì¡°í•©ì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()

            preview_df = pd.DataFrame(preview_list)
            st.markdown("### âœ… ì €ì¥ë  í•™ìŠµ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            st.dataframe(preview_df)

            if st.button("ğŸ’¾ GitHubì— ì €ì¥í•˜ê¸°", key="save_tab2_multi"):
                csv_path = "ML_asos_dataset_by_gu.csv"
                if os.path.exists(csv_path):
                    try:
                        existing = pd.read_csv(csv_path, encoding="utf-8-sig")
                    except UnicodeDecodeError:
                        existing = pd.read_csv(csv_path, encoding="cp949")
                    for row in preview_list:
                        existing = existing[~((existing["ì¼ì"] == row["ì¼ì"]) & (existing["ìì¹˜êµ¬"] == row["ìì¹˜êµ¬"]))]
                    df_all = pd.concat([existing, preview_df], ignore_index=True)
                else:
                    df_all = preview_df

                df_all.to_csv(csv_path, index=False, encoding="utf-8-sig")

                with open(csv_path, "rb") as f:
                    content = f.read()
                b64_content = base64.b64encode(content).decode("utf-8")

                api_url = f"https://api.github.com/repos/{GITHUB_USERNAME}/{GITHUB_REPO}/contents/{GITHUB_FILENAME}"
                r = requests.get(api_url, headers={"Authorization": f"Bearer {GITHUB_TOKEN}"})
                sha = r.json().get("sha") if r.status_code == 200 else None

                payload = {
                    "message": f"Update {GITHUB_FILENAME} with new data for {region} ({len(preview_list)} entries)",
                    "content": b64_content,
                    "branch": GITHUB_BRANCH
                }
                if sha:
                    payload["sha"] = sha

                headers = {
                    "Authorization": f"Bearer {GITHUB_TOKEN}",
                    "Accept": "application/vnd.github+json"
                }
                r = requests.put(api_url, headers=headers, json=payload)
                if r.status_code in [200, 201]:
                    st.success("âœ… GitHub ì €ì¥ ì™„ë£Œ")
                    st.info(f"ğŸ”— [GitHubì—ì„œ ë³´ê¸°](https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}/blob/{GITHUB_BRANCH}/{GITHUB_FILENAME})")
                else:
                    st.warning(f"âš ï¸ GitHub ì €ì¥ ì‹¤íŒ¨: {r.status_code} {r.text[:200]}")

        except Exception as e:
            st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

with tab3:
    st.header("ğŸ“ ìì¹˜êµ¬ë³„ í”¼í•´ì ìˆ˜ ë° ë³´ìƒê¸ˆ ê³„ì‚°í•˜ê¸°")

    def calculate_social_index(row):
        return (
            0.5 * row["ê³ ë ¹ìë¹„ìœ¨"] +
            0.3 * row["ì•¼ì™¸ê·¼ë¡œìë¹„ìœ¨"] +
            0.2 * row["ì—´ì¾Œì ì·¨ì•½ì¸êµ¬ë¹„ìœ¨"]
        )

    def standardize_column(df, col):
        min_val = df[col].min()
        max_val = df[col].max()
        range_val = max_val - min_val if max_val != min_val else 1
        return (df[col] - min_val) / range_val

    def calculate_environment_index(row):
        return (
            0.5 * row["ì—´ì„¬ì§€ìˆ˜_std"] +
            0.3 * (1 - row["ë…¹ì§€ìœ¨_std"]) +
            0.2 * (1 - row["ëƒ‰ë°©ë³´ê¸‰ë¥ _std"])
        )

    def calculate_damage_score(s, e, pred_count, real_count):
        P_pred = min(pred_count / 25, 1.0)
        P_real = 1.0 if real_count >= 1 else 0.0
        return 100 * (0.4 * s + 0.3 * e + 0.2 * P_pred + 0.1 * P_real)

    def score_to_grade(score):
        if score < 20: return "ğŸŸ¢ ë§¤ìš° ë‚®ìŒ"
        elif score < 30: return "ğŸŸ¡ ë‚®ìŒ"
        elif score < 40: return "ğŸŸ  ë³´í†µ"
        elif score < 50: return "ğŸ”´ ë†’ìŒ"
        else: return "ğŸ”¥ ë§¤ìš° ë†’ìŒ"

    def calc_payout(score):
        if score < 30: return 0
        elif score < 40: return 5000
        elif score < 50: return 10000
        else: return 20000

    try:
        selected_date = st.date_input("ğŸ“… ë¶„ì„ ë‚ ì§œ ì„ íƒ", datetime.date.today())
        ymd = selected_date.strftime("%Y-%m-%d")

        def load_csv_with_fallback(path):
            for enc in ["utf-8-sig", "cp949", "euc-kr"]:
                try:
                    return pd.read_csv(path, encoding=enc)
                except UnicodeDecodeError:
                    continue
            raise UnicodeDecodeError(f"âŒ ì¸ì½”ë”© ì‹¤íŒ¨: {path}")

        ml_data = load_csv_with_fallback("ML_asos_dataset.csv")
        static_data = load_csv_with_fallback("seoul_static_data.csv")

        merged_all = pd.merge(static_data, ml_data, on="ìì¹˜êµ¬", how="left")
        merged_all = merged_all[merged_all["ì¼ì"] == ymd].copy()

        if merged_all.empty:
            st.warning("â—ï¸ì„ íƒí•œ ë‚ ì§œì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        df_total = load_csv_with_fallback("ML_asos_total_prediction.csv")
        pred_row = df_total[df_total["ì¼ì"] == ymd]

        if pred_row.empty:
            st.warning(f"âš ï¸ {ymd} ì˜ˆì¸¡ê°’ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. tab1ì—ì„œ ë¨¼ì € ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
            st.stop()

        seoul_pred = float(pred_row["ì„œìš¸ì‹œì˜ˆì¸¡í™˜ììˆ˜"].values[0])
        merged_all["ì˜ˆì¸¡í™˜ììˆ˜"] = seoul_pred

        merged_all["S"] = merged_all.apply(calculate_social_index, axis=1)
        for col in ["ì—´ì„¬ì§€ìˆ˜", "ë…¹ì§€ìœ¨", "ëƒ‰ë°©ë³´ê¸‰ë¥ "]:
            merged_all[f"{col}_std"] = standardize_column(merged_all, col)
        merged_all["E"] = merged_all.apply(calculate_environment_index, axis=1)

        merged_all["í”¼í•´ì ìˆ˜"] = merged_all.apply(
            lambda row: calculate_damage_score(
                row["S"], row["E"], row["ì˜ˆì¸¡í™˜ììˆ˜"], row["í™˜ììˆ˜"]
            ),
            axis=1
        )
        merged_all["ìœ„í—˜ë“±ê¸‰"] = merged_all["í”¼í•´ì ìˆ˜"].apply(score_to_grade)
        merged_all["ë³´ìƒê¸ˆ"] = merged_all["í”¼í•´ì ìˆ˜"].apply(calc_payout)

        selected_gu = st.selectbox("ğŸ˜ï¸ ìì¹˜êµ¬ ì„ íƒ", sorted(merged_all["ìì¹˜êµ¬"].unique()))
        merged = merged_all[merged_all["ìì¹˜êµ¬"] == selected_gu].copy()

        subs_count = st.number_input(f"{selected_gu} ê°€ì…ì ìˆ˜", min_value=0, step=1, key="subs_tab3")
        merged["ê°€ì…ììˆ˜"] = subs_count
        merged["ì˜ˆìƒì´ë³´ìƒê¸ˆ"] = merged["ë³´ìƒê¸ˆ"] * subs_count
        st.success(f"ğŸ’° ì˜ˆìƒ ë³´ìƒê¸ˆì•¡: {int(merged['ì˜ˆìƒì´ë³´ìƒê¸ˆ'].values[0]):,}ì›")

        show_cols = ["ìì¹˜êµ¬", "í”¼í•´ì ìˆ˜", "ìœ„í—˜ë“±ê¸‰", "ë³´ìƒê¸ˆ", "ê°€ì…ììˆ˜", "ì˜ˆìƒì´ë³´ìƒê¸ˆ"]
        st.dataframe(merged[show_cols], use_container_width=True)

        st.markdown("### ğŸ“Š í”¼í•´ì ìˆ˜ ë¶„í¬")
        st.bar_chart(data=merged_all.set_index("ìì¹˜êµ¬")["í”¼í•´ì ìˆ˜"])

        # ë¡œê·¸ ë° ë‹¤ìš´ë¡œë“œ
        row = merged.iloc[0]
        s_val = row["S"]
        e_val = row["E"]
        pred_count = row["ì˜ˆì¸¡í™˜ììˆ˜"]
        real_count = row["í™˜ììˆ˜"]
        score = row["í”¼í•´ì ìˆ˜"]
        P_pred = min(pred_count / 25, 1.0)
        P_real = 1.0 if real_count >= 1 else 0.0

        single_log = f"""
[í”¼í•´ì ìˆ˜ ê³„ì‚° ë¡œê·¸ - {selected_gu} / {ymd}]
--------------------------------------------------
[S ê³„ì‚°]
- ê³ ë ¹ìë¹„ìœ¨              = {row['ê³ ë ¹ìë¹„ìœ¨']:.4f}
- ì•¼ì™¸ê·¼ë¡œìë¹„ìœ¨          = {row['ì•¼ì™¸ê·¼ë¡œìë¹„ìœ¨']:.4f}
- ì—´ì¾Œì ì·¨ì•½ì¸êµ¬ë¹„ìœ¨      = {row['ì—´ì¾Œì ì·¨ì•½ì¸êµ¬ë¹„ìœ¨']:.4f}
=> S = {s_val:.4f}

[E ê³„ì‚°]
- ì—´ì„¬ì§€ìˆ˜ (í‘œì¤€í™”)       = {row['ì—´ì„¬ì§€ìˆ˜_std']:.4f}
- ë…¹ì§€ìœ¨ (í‘œì¤€í™”)         = {row['ë…¹ì§€ìœ¨_std']:.4f}
- ëƒ‰ë°©ë³´ê¸‰ë¥  (í‘œì¤€í™”)     = {row['ëƒ‰ë°©ë³´ê¸‰ë¥ _std']:.4f}
=> E = {e_val:.4f}

[í™˜ì ìˆ˜ ì…ë ¥ê°’]
- ì˜ˆì¸¡í™˜ììˆ˜              = {pred_count:.1f}ëª… â†’ ì •ê·œí™” = {P_pred:.3f}
- ì‹¤ì œí™˜ììˆ˜              = {real_count:.1f}ëª… â†’ ì •ê·œí™” = {P_real:.1f}

[ìµœì¢… í”¼í•´ì ìˆ˜ ê³„ì‚°]
í”¼í•´ì ìˆ˜ = 100 Ã— (0.4Ã—{s_val:.4f} + 0.3Ã—{e_val:.4f} + 0.2Ã—{P_pred:.4f} + 0.1Ã—{P_real:.1f})  
         = {score:.2f}
--------------------------------------------------
"""
        with st.expander(f"ğŸ” {selected_gu} ë””ë²„ê¹… ë¡œê·¸"):
            st.code(single_log, language="text")
            st.download_button(
                label="ğŸ“„ í˜„ì¬ ìì¹˜êµ¬ ë””ë²„ê¹… ë¡œê·¸ ë‹¤ìš´ë¡œë“œ",
                data=single_log.encode("utf-8-sig"),
                file_name=f"í”¼í•´ì ìˆ˜_ë””ë²„ê¹…_{ymd}_{selected_gu}.txt",
                mime="text/plain"
            )

        all_debug_logs = ""
        for _, row in merged_all.iterrows():
            s_val = row["S"]
            e_val = row["E"]
            pred_count = row["ì˜ˆì¸¡í™˜ììˆ˜"]
            real_count = row["í™˜ììˆ˜"]
            score = row["í”¼í•´ì ìˆ˜"]
            P_pred = min(pred_count / 25, 1.0)
            P_real = 1.0 if real_count >= 1 else 0.0

            log = f"""
[í”¼í•´ì ìˆ˜ ê³„ì‚° ë¡œê·¸ - {row['ìì¹˜êµ¬']} / {ymd}]
--------------------------------------------------
[S ê³„ì‚°]
- ê³ ë ¹ìë¹„ìœ¨              = {row['ê³ ë ¹ìë¹„ìœ¨']:.4f}
- ì•¼ì™¸ê·¼ë¡œìë¹„ìœ¨          = {row['ì•¼ì™¸ê·¼ë¡œìë¹„ìœ¨']:.4f}
- ì—´ì¾Œì ì·¨ì•½ì¸êµ¬ë¹„ìœ¨      = {row['ì—´ì¾Œì ì·¨ì•½ì¸êµ¬ë¹„ìœ¨']:.4f}
=> S = {s_val:.4f}

[E ê³„ì‚°]
- ì—´ì„¬ì§€ìˆ˜ (í‘œì¤€í™”)       = {row['ì—´ì„¬ì§€ìˆ˜_std']:.4f}
- ë…¹ì§€ìœ¨ (í‘œì¤€í™”)         = {row['ë…¹ì§€ìœ¨_std']:.4f}
- ëƒ‰ë°©ë³´ê¸‰ë¥  (í‘œì¤€í™”)     = {row['ëƒ‰ë°©ë³´ê¸‰ë¥ _std']:.4f}
=> E = {e_val:.4f}

[í™˜ì ìˆ˜ ì…ë ¥ê°’]
- ì˜ˆì¸¡í™˜ììˆ˜              = {pred_count:.1f}ëª… â†’ ì •ê·œí™” = {P_pred:.3f}
- ì‹¤ì œí™˜ììˆ˜              = {real_count:.1f}ëª… â†’ ì •ê·œí™” = {P_real:.1f}

[ìµœì¢… í”¼í•´ì ìˆ˜ ê³„ì‚°]
í”¼í•´ì ìˆ˜ = 100 Ã— (0.4Ã—{s_val:.4f} + 0.3Ã—{e_val:.4f} + 0.2Ã—{P_pred:.4f} + 0.1Ã—{P_real:.1f})  
         = {score:.2f}
--------------------------------------------------
"""
            all_debug_logs += log + "\n"

        st.download_button(
            label="ğŸ“¥ ì „ì²´ ìì¹˜êµ¬ ë””ë²„ê¹… ë¡œê·¸ ë‹¤ìš´ë¡œë“œ",
            data=all_debug_logs.encode("utf-8-sig"),
            file_name=f"ì „ì²´_í”¼í•´ì ìˆ˜_ë””ë²„ê¹…_{ymd}.txt",
            mime="text/plain"
        )

        with st.expander("ğŸ“˜ í”¼í•´ì ìˆ˜ êµ¬ì„± ìš”ì†Œ ì•ˆë‚´"):
            st.markdown("""
> ğŸ§“ **S (ì‚¬íšŒì  ì·¨ì•½ì„± ì§€ìˆ˜)**  
ê³ ë ¹ì, ì•¼ì™¸ ê·¼ë¡œì, ì—´ì— ì·¨ì•½í•œ ì¸êµ¬ì˜ ë¹„ìœ¨ì„ ë°˜ì˜í•œ ì§€ìˆ˜ì…ë‹ˆë‹¤.

> ğŸŒ **E (í™˜ê²½ì  ì·¨ì•½ì„± ì§€ìˆ˜)**  
ì—´ì„¬ í˜„ìƒ, ë…¹ì§€ìœ¨, ëƒ‰ë°© ë³´ê¸‰ë¥  ë“±ì„ í‘œì¤€í™”í•˜ì—¬ ë°˜ì˜í•œ ì§€ìˆ˜ì…ë‹ˆë‹¤.

> ğŸ“ˆ **ì˜ˆì¸¡í™˜ììˆ˜**  
ì„œìš¸ì‹œ ì „ì²´ ì˜ˆì¸¡ í™˜ì ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©°, ëª¨ë“  ìì¹˜êµ¬ì— ë™ì¼í•˜ê²Œ ì ìš©ë©ë‹ˆë‹¤. (25ëª… ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”, ìµœëŒ€ 1.0)

> ğŸ“‰ **ì‹¤ì œí™˜ììˆ˜**  
1ëª… ì´ìƒ ë°œìƒ ì‹œ 1.0, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0.0ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.

> ğŸ§® **í”¼í•´ì ìˆ˜ ê³„ì‚°ì‹**  
í”¼í•´ì ìˆ˜ = 100 Ã— (0.4 Ã— S + 0.3 Ã— E + 0.2 Ã— P_pred + 0.1 Ã— P_real)
""")

    except Exception as e:
        st.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
