import streamlit as st
import pandas as pd
import datetime
import requests
import os
import base64
import io
from urllib.parse import unquote
import subprocess
import sys


from utils import (
    get_weather, get_asos_weather, get_risk_level,
    calculate_avg_temp, region_to_stn_id
)
from model_utils import predict_from_weather

# ----------------------- ì„¤ì • -----------------------
st.set_page_config(layout="centered")

KMA_API_KEY = unquote(st.secrets["KMA"]["API_KEY"])
ASOS_API_KEY = unquote(st.secrets["ASOS"]["API_KEY"])
GITHUB_USERNAME = st.secrets["GITHUB"]["USERNAME"]
GITHUB_REPO = st.secrets["GITHUB"]["REPO"]
GITHUB_BRANCH = st.secrets["GITHUB"]["BRANCH"]
GITHUB_TOKEN = st.secrets["GITHUB"]["TOKEN"]
GITHUB_FILENAME = "ML_asos_dataset.csv"

# ----------------------- UI ì‹œì‘ -----------------------
st.title("HeatAI")
tab1, tab2, tab3 = st.tabs(["í•™ìŠµ ë°ì´í„° ì…ë ¥", "í™˜ì ìˆ˜ ì§€í‘œ ì‚°ì¶œ", "í”¼í•´ì ìˆ˜ ê³„ì‚° ë° ë³´ìƒ"])

with tab1:
    with st.expander("ì´ íƒ­ì—ì„œëŠ” ë¬´ì—‡ì„ í•˜ë‚˜ìš”?"):
        st.markdown("""
        ì´ íƒ­ì€ AI ëª¨ë¸ì˜ í•™ìŠµì„ ìœ„í•œ **ì‚¬ìš©ì ì…ë ¥ í•™ìŠµ ë°ì´í„°**ë¥¼ ì¶”ê°€í•˜ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 
        ì‚¬ìš©ìëŠ” ì§ˆë³‘ê´€ë¦¬ì²­ ì˜¨ì—´ì§ˆí™˜ì í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, 
        ì„œìš¸íŠ¹ë³„ì‹œ ìì¹˜êµ¬ë³„ **ì‹¤ì œ í™˜ì ìˆ˜ì™€ í•´ë‹¹ ì¼ìì˜ ê¸°ìƒ ì •ë³´**ë¥¼ ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        ì´ ë°ì´í„°ëŠ” ê¸°ì¡´ì˜ **ê³¼ê±° ê¸°ì´ˆ í•™ìŠµ ë°ì´í„°ì…‹(2021~2024ë…„ 7Â·8ì›”)**ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì–´ 
        **XGBoost ê¸°ë°˜ ëª¨ë¸ì„ ìµœì‹  ìƒíƒœë¡œ ì¬í•™ìŠµ**í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤.

        ---
        **ìˆ˜í–‰ ë‚´ìš© ìš”ì•½**:

        1. ì§ˆë³‘ì²­ ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ â†’ ìì¹˜êµ¬ë³„ í™˜ì ìˆ˜ ìë™ ì¶”ì¶œ
        2. ì„ íƒ ë‚ ì§œì˜ ê¸°ìƒì •ë³´ ìë™ ìˆ˜ì§‘ (ASOS API)
        3. ìì¹˜êµ¬ë³„ í™˜ììˆ˜ + ê¸°ìƒì •ë³´ â†’ í•™ìŠµìš© ë°ì´í„°í”„ë ˆì„ ìƒì„±
        4. `ML_asos_dataset.csv`ì— ì €ì¥ í›„ ìë™ ì¬í•™ìŠµ ìˆ˜í–‰

        ì´ ê³¼ì •ì€ TAB2ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ë©°, TAB3ì˜ í”¼í•´ì ìˆ˜ ì‹ ë¢°ë„ë¥¼ ë†’ì—¬ì¤ë‹ˆë‹¤.
        ì•„ë˜ ë§í¬ì—ì„œ ì§ˆë³‘ì²­ì˜ ì˜¨ì—´ì§ˆí™˜ì ì—‘ì…€ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•´ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.  
        [ì˜¨ì—´ì§ˆí™˜ ì‘ê¸‰ì‹¤ê°ì‹œì²´ê³„ ë‹¤ìš´ë¡œë“œ](https://www.kdca.go.kr/board/board.es?mid=a20205030102&bid=0004&&cg_code=C01)
""")

    region = st.selectbox("ê´‘ì—­ì‹œë„ ì„ íƒ", ["ì„œìš¸íŠ¹ë³„ì‹œ"], key="region_tab1")

    all_gus = [
        'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ê´‘ì§„êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ì¤‘ë‘êµ¬', 'ì„±ë¶êµ¬', 'ê°•ë¶êµ¬', 'ë„ë´‰êµ¬',
        'ë…¸ì›êµ¬', 'ì€í‰êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ë§ˆí¬êµ¬', 'ì–‘ì²œêµ¬', 'ê°•ì„œêµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ì˜ë“±í¬êµ¬',
        'ë™ì‘êµ¬', 'ê´€ì•…êµ¬', 'ì„œì´ˆêµ¬', 'ê°•ë‚¨êµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬'
    ]
    gus = st.multiselect("ìì¹˜êµ¬ ì„ íƒ (ì„ íƒí•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´)", all_gus, key="gu_tab1_multi")
    if not gus:
        gus = all_gus

    min_record_date = datetime.date(2025, 7, 1)
    max_record_date = datetime.date.today() - datetime.timedelta(days=1)

    date_selected = st.date_input(
        "ì €ì¥í•  ë‚ ì§œ ì„ íƒ", 
        value=max_record_date, 
        min_value=min_record_date, 
        max_value=max_record_date,
        key="date_tab1"
    )

    uploaded_file = st.file_uploader("ì§ˆë³‘ì²­ í™˜ììˆ˜ íŒŒì¼ ì—…ë¡œë“œ (.xlsx, ì‹œíŠ¸ëª…: ì„œìš¸íŠ¹ë³„ì‹œ)", type=["xlsx"], key="upload_tab1")

    if uploaded_file and date_selected:
        try:
            df_raw = pd.read_excel(uploaded_file, sheet_name="ì„œìš¸íŠ¹ë³„ì‹œ", header=None)
            districts = df_raw.iloc[0, 1::2].tolist()
            dates = df_raw.iloc[3:, 0].tolist()
            df_values = df_raw.iloc[3:, 1::2]
            df_values.columns = districts
            df_values.insert(0, "ì¼ì", dates)
            df_long = df_values.melt(id_vars=["ì¼ì"], var_name="ìì¹˜êµ¬", value_name="í™˜ììˆ˜")
            df_long["ì¼ì"] = pd.to_datetime(df_long["ì¼ì"], errors="coerce").dt.strftime("%Y-%m-%d")
            df_long["í™˜ììˆ˜"] = pd.to_numeric(df_long["í™˜ììˆ˜"], errors="coerce").fillna(0).astype(int)
            df_long["ì§€ì—­"] = "ì„œìš¸íŠ¹ë³„ì‹œ"

            preview_list = []
            ymd = date_selected.strftime("%Y-%m-%d")

            weather = get_asos_weather(region, ymd.replace("-", ""), ASOS_API_KEY)
            tmx = weather.get("TMX", 0)
            tmn = weather.get("TMN", 0)
            reh = weather.get("REH", 0)
            pred, avg_temp, heat_index, input_df = predict_from_weather(tmx, tmn, reh)


            for gu in gus:
                selected = df_long[(df_long["ì¼ì"] == ymd) & (df_long["ìì¹˜êµ¬"] == gu)]
                if selected.empty:
                    st.warning(f"{ymd} {gu} í™˜ììˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                í™˜ììˆ˜ = int(selected["í™˜ììˆ˜"].values[0])
                preview_list.append({
                    "ì¼ì": ymd,
                    "ì§€ì—­": region,
                    "ìì¹˜êµ¬": gu,
                    "ìµœê³ ì²´ê°ì˜¨ë„(Â°C)": heat_index,
                    "ìµœê³ ê¸°ì˜¨(Â°C)": tmx,
                    "í‰ê· ê¸°ì˜¨(Â°C)": avg_temp,
                    "ìµœì €ê¸°ì˜¨(Â°C)": tmn,
                    "í‰ê· ìƒëŒ€ìŠµë„(%)": reh,
                    "í™˜ììˆ˜": í™˜ììˆ˜
                })

            if not preview_list:
                st.warning("ì„ íƒí•œ ë‚ ì§œì™€ ìì¹˜êµ¬ ì¡°í•©ì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()

            preview_df = pd.DataFrame(preview_list)
            st.markdown("#### ì €ì¥ë  í•™ìŠµ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            st.dataframe(preview_df)

            if st.button("GitHubì— ì €ì¥í•˜ê³  ëª¨ë¸ ì¬í•™ìŠµí•˜ê¸°", key="save_and_train_tab1"):
                csv_path = "ML_asos_dataset.csv"

                if os.path.exists(csv_path):
                    try:
                        existing = pd.read_csv(csv_path, encoding="utf-8-sig")
                    except UnicodeDecodeError:
                        existing = pd.read_csv(csv_path, encoding="cp949")
                else:
                    existing = pd.DataFrame()

                merge_keys = ["ì¼ì", "ìì¹˜êµ¬"]
                if not existing.empty:
                    existing = existing[~existing.set_index(merge_keys).index.isin(preview_df.set_index(merge_keys).index)]
                df_all = pd.concat([existing, preview_df], ignore_index=True)
                df_all.to_csv(csv_path, index=False, encoding="utf-8-sig")
                st.success("í•™ìŠµ ë°ì´í„° ì €ì¥ ì™„ë£Œ (ë¡œì»¬)")

                try:
                    with open(csv_path, "rb") as f:
                        content = f.read()
                    b64_content = base64.b64encode(content).decode("utf-8")

                    api_url = f"https://api.github.com/repos/{GITHUB_USERNAME}/{GITHUB_REPO}/contents/{GITHUB_FILENAME}"
                    r = requests.get(api_url, headers={"Authorization": f"Bearer {GITHUB_TOKEN}"})
                    sha = r.json().get("sha") if r.status_code == 200 else None

                    payload = {
                        "message": f"Update {GITHUB_FILENAME} with new data for {region} ({len(preview_list)} entries)",
                        "content": b64_content,
                        "branch": GITHUB_BRANCH
                    }
                    if sha:
                        payload["sha"] = sha

                    headers = {
                        "Authorization": f"Bearer {GITHUB_TOKEN}",
                        "Accept": "application/vnd.github+json"
                    }
                    r = requests.put(api_url, headers=headers, json=payload)
                    if r.status_code in [200, 201]:
                        st.success("GitHub ì €ì¥ ì™„ë£Œ")
                        st.info(f"[GitHubì—ì„œ ë³´ê¸°](https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}/blob/{GITHUB_BRANCH}/{GITHUB_FILENAME})")
                    else:
                        st.warning(f"GitHub ì €ì¥ ì‹¤íŒ¨: {r.status_code} {r.text[:200]}")

                except Exception as e:
                    st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    st.stop()

                st.info("ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì¬í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤...")
                try:
                    result = subprocess.run([sys.executable, "train_model.py"], capture_output=True, text=True, check=True)
                    st.success("ëª¨ë¸ ì¬í•™ìŠµ ì™„ë£Œ")
                    st.text_area("í•™ìŠµ ë¡œê·¸", result.stdout, height=300)
                except subprocess.CalledProcessError as e:
                    st.error("ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
                    st.text_area("ì˜¤ë¥˜ ë¡œê·¸", e.stderr or str(e), height=300)

        except Exception as e:
            st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

with tab2:
    def get_last_year_patient_count(current_date, region):
        try:
            last_year_date = (current_date - datetime.timedelta(days=365)).strftime("%Y-%m-%d")
            static_file = "ML_static_dataset.csv"
            df_all = pd.read_csv(static_file, encoding="cp949")

            if "ì¼ì‹œ" in df_all.columns and pd.api.types.is_numeric_dtype(df_all["ì¼ì‹œ"]):
                df_all["ì¼ì‹œ"] = pd.to_datetime("1899-12-30") + pd.to_timedelta(df_all["ì¼ì‹œ"], unit="D")
                df_all["ì¼ì"] = df_all["ì¼ì‹œ"].dt.strftime("%Y-%m-%d")
            elif "ì¼ì" not in df_all.columns and "ì¼ì‹œ" in df_all.columns:
                 df_all["ì¼ì"] = pd.to_datetime(df_all["ì¼ì‹œ"]).dt.strftime("%Y-%m-%d")


            cond = (df_all["ì¼ì"] == last_year_date) & (df_all["ê´‘ì—­ìì¹˜ë‹¨ì²´"] == region)
            row = df_all[cond]
            return int(row["í™˜ììˆ˜"].values[0]) if not row.empty else None

        except Exception as e:
            st.warning(f"ì‘ë…„ í™˜ììˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return None

    with st.expander("ì´ íƒ­ì—ì„œëŠ” ë¬´ì—‡ì„ í•˜ë‚˜ìš”?"):
        st.markdown("""
        ì´ íƒ­ì€ ì„ íƒí•œ ë‚ ì§œì˜ **ê¸°ìƒ ì¡°ê±´(ì˜ˆë³´ ë˜ëŠ” ì‹¤ì¸¡)**ì„ ê¸°ë°˜ìœ¼ë¡œ,  
        AI ëª¨ë¸ì´ **ì„œìš¸ì‹œ ì „ì²´ ì˜ˆìƒ ì˜¨ì—´ í™˜ì ìˆ˜(P_pred)**ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.

        ì‚¬ìš©ë˜ëŠ” AI ëª¨ë¸ì€:
        - **2021~2024ë…„ì˜ ê³¼ê±° ê¸°ì´ˆ í•™ìŠµ ë°ì´í„°**ì™€,
        - tab1ì„ í†µí•´ ì…ë ¥ëœ **ì‚¬ìš©ì ì…ë ¥ í•™ìŠµ ë°ì´í„°**ë¥¼ ê²°í•©í•˜ì—¬ í•™ìŠµëœ XGBoost ê¸°ë°˜ ëª¨ë¸ì…ë‹ˆë‹¤.

        ---
        **ìˆ˜í–‰ ë‚´ìš© ìš”ì•½**:

        1. **ë‚ ì§œì— ë”°ë¼ ê¸°ìƒ ë°ì´í„° ì…ë ¥ ë°©ì‹ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤**:
           - ì˜¤ëŠ˜ ì´ì „ ë‚ ì§œ: ASOS ì‹¤ì¸¡ ê¸°ìƒ ë°ì´í„°
           - ì˜¤ëŠ˜ ì´í›„ ë‚ ì§œ: ê¸°ìƒì²­ ë‹¨ê¸°ì˜ˆë³´ API ì˜ˆë³´ ë°ì´í„°

        2. ì…ë ¥ëœ ê¸°ìƒ ì¡°ê±´ (TMX, TMN, REH ë“±)ì„ ê¸°ë°˜ìœ¼ë¡œ,
           ëª¨ë¸ì€ **ê²°ì • íŠ¸ë¦¬ ê¸°ë°˜ ì˜ˆì¸¡ ê²½ë¡œë¥¼ ë”°ë¼ P_predë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ê³„ì‚°**í•©ë‹ˆë‹¤.

        3. ì˜ˆì¸¡ ê²°ê³¼ëŠ” `ML_asos_total_prediction.csv`ì— ì €ì¥ë˜ë©°,
           GitHubì— ìë™ ì—…ë¡œë“œë˜ì–´ tab3ì—ì„œ í”¼í•´ì ìˆ˜ ê³„ì‚°ì— ì¦‰ì‹œ ì—°ë™ë©ë‹ˆë‹¤.

    # ë‚ ì§œ ì„ íƒ ë²”ìœ„ ì„¤ì •
    min_pred_date = datetime.date(2025, 7, 1)
    max_pred_date = datetime.date(2025, 8, 31)

    # ì§€ì—­ ë° ë‚ ì§œ ì„ íƒ UI
    region = st.selectbox("ì§€ì—­ ì„ íƒ", list(region_to_stn_id.keys()), key="region_tab2")
    date_selected = st.date_input("ë‚ ì§œ ì„ íƒ", value=min_pred_date, min_value=min_pred_date, max_value=max_pred_date, key="date_tab2")

    # ì˜ˆì¸¡ ë²„íŠ¼ í´ë¦­ ì‹œ ì‹¤í–‰
    if st.button("P_pred ì¶”ì •í•˜ê¸°", key="predict_tab2"):
        today = datetime.date.today()

        if date_selected >= today:
            weather, base_date, base_time = get_weather(region, date_selected, KMA_API_KEY)
        else:
            ymd = date_selected.strftime("%Y%m%d")
            weather = get_asos_weather(region, ymd, ASOS_API_KEY)

        if not weather:
            st.error("ê¸°ìƒ ì •ë³´ ì—†ìŒ")
            st.stop()

        tmx = weather.get("TMX", 0)
        tmn = weather.get("TMN", 0)
        reh = weather.get("REH", 0)

        pred, avg_temp, heat_index, input_df = predict_from_weather(tmx, tmn, reh)
        risk = get_risk_level(pred)

        with st.expander("ì…ë ¥ê°’ í™•ì¸"):
            st.dataframe(input_df)

        st.markdown("####P_pred")
        c1, c2 = st.columns(2)
        c1.metric("ì˜ˆì¸¡ í™˜ì ìˆ˜", f"{pred:.2f}ëª…")
        c2.metric("ìœ„í—˜ ë“±ê¸‰", risk)

        last_year_count = get_last_year_patient_count(date_selected, region)
        if last_year_count is not None:
            delta = pred - last_year_count
            st.markdown(
                    f"**ì „ë…„ë„({(date_selected - datetime.timedelta(days=365)).strftime('%Y-%m-%d')}) ë™ì¼ ë‚ ì§œ í™˜ììˆ˜**: **{last_year_count}ëª…**  \n"
                    f"**ì „ë…„ ëŒ€ë¹„ ì¦ê°€**: {'+' if delta >= 0 else ''}{delta:.1f}ëª…"
)
        else:
            st.info("ì „ë…„ë„ ë™ì¼ ë‚ ì§œì˜ í™˜ì ìˆ˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        SAVE_FILE = "ML_asos_total_prediction.csv"
        today_str = date_selected.strftime("%Y-%m-%d")

        try:
            df_total = pd.read_csv(SAVE_FILE, encoding="utf-8-sig")
        except FileNotFoundError:
            df_total = pd.DataFrame(columns=["ì¼ì", "ì„œìš¸ì‹œì˜ˆì¸¡í™˜ììˆ˜"])

        new_row = pd.DataFrame([{ "ì¼ì": today_str, "ì„œìš¸ì‹œì˜ˆì¸¡í™˜ììˆ˜": round(pred, 2) }])
        df_total = pd.concat([df_total, new_row], ignore_index=True)
        df_total.to_csv(SAVE_FILE, index=False, encoding="utf-8-sig")
        st.success(f"ì˜ˆì¸¡ê°’ì´ '{SAVE_FILE}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        with open(SAVE_FILE, "rb") as f:
            content = f.read()
        b64_content = base64.b64encode(content).decode("utf-8")

        api_url = f"https://api.github.com/repos/{GITHUB_USERNAME}/{GITHUB_REPO}/contents/{SAVE_FILE}"
        r = requests.get(api_url, headers={"Authorization": f"Bearer {GITHUB_TOKEN}"})
        sha = r.json().get("sha") if r.status_code == 200 else None

        payload = {
            "message": f"[tab2] {date_selected} ì˜ˆì¸¡ê°’ ì €ì¥",
            "content": b64_content,
            "branch": GITHUB_BRANCH
        }
        if sha:
            payload["sha"] = sha

        headers = {
            "Authorization": f"Bearer {GITHUB_TOKEN}",
            "Accept": "application/vnd.github+json"
        }
        r = requests.put(api_url, headers=headers, json=payload)

        if r.status_code in [200, 201]:
            st.success("GitHubì— ì˜ˆì¸¡ê°’ ì €ì¥ ì™„ë£Œ")
            st.info(f"ğŸ”— [GitHubì—ì„œ í™•ì¸í•˜ê¸°](https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}/blob/{GITHUB_BRANCH}/{SAVE_FILE})")
        else:
            st.warning(f"GitHub ì €ì¥ ì‹¤íŒ¨: {r.status_code} / {r.text[:200]}")


with tab3:
    with st.expander("ì´ íƒ­ì—ì„œëŠ” ë¬´ì—‡ì„ í•˜ë‚˜ìš”?"):
        st.markdown("""
        ì´ íƒ­ì€ HeatAIì˜ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ, 
        **ì˜ˆì¸¡ í™˜ì ìˆ˜(P_pred)**, **ì‹¤ì œ í™˜ì ìˆ˜(P_real)**, 
        ê·¸ë¦¬ê³  **ìì¹˜êµ¬ë³„ ê³ ì •ëœ ì§€í‘œ(S, E)**ì™€ **í­ì—¼ ì§€ì†ì„±(H)**ì„ í™œìš©í•´
        **ìì¹˜êµ¬ë³„ í”¼í•´ì ìˆ˜ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ê³„ì‚°**í•˜ê³ , 
        ìœ„í—˜ ë“±ê¸‰ê³¼ **ì˜ˆìƒ ë³´ìƒê¸ˆì•¡**ê¹Œì§€ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

        ---
        **ìˆ˜í–‰ ë‚´ìš© ìš”ì•½**:

        1. ì‚¬íšŒì  ì§€í‘œ(S): ê³ ë ¹ì ë¹„ìœ¨, ì•¼ì™¸ê·¼ë¡œì ë¹„ìœ¨, ì—´ì·¨ì•½ ì¸êµ¬ ë¹„ìœ¨
        2. í™˜ê²½ì  ì§€í‘œ(E): ì—´ì„¬ì§€ìˆ˜, ë…¹ì§€ìœ¨, ëƒ‰ë°©ë³´ê¸‰ë¥  (í‘œì¤€í™”)
        3. P_pred: tab2ì—ì„œ ì˜ˆì¸¡ëœ ì„œìš¸ì‹œ ì „ì²´ ì˜¨ì—´ í™˜ì ìˆ˜ 
        4. P_real: tab1ì—ì„œ ì…ë ¥ëœ ìì¹˜êµ¬ë³„ ì‹¤ì œ ì˜¨ì—´ í™˜ì ìˆ˜ 
        5. H: ìµœê·¼ 7ì¼ê°„ ìµœê³ ì²´ê°ì˜¨ë„ ê¸°ë°˜ í­ì—¼ ì§€ì† ê°€ì¤‘ì¹˜ ê³„ì‚°

        **ì‚¬ì „ í”¼í•´ì ìˆ˜** = 100 Ã— (0.25Ã—S + 0.25Ã—E + 0.5Ã—P_pred) Ã— H  
        **ì‚¬í›„ í”¼í•´ì ìˆ˜** = 100 Ã— (0.2Ã—S + 0.2Ã—E + 0.5Ã—P_pred + 0.1Ã—P_real) Ã— H

        ê°€ì…ì ìˆ˜ë¥¼ ì…ë ¥í•˜ë©´ ì˜ˆìƒ ì´ ë³´ìƒê¸ˆì•¡ë„ ê³„ì‚°ë˜ë©°, 
        ëª¨ë“  ê³„ì‚° ê²°ê³¼ëŠ” ë””ë²„ê¹… ë¡œê·¸ë¡œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        """)

    # í•¨ìˆ˜ ì •ì˜
    def calculate_heatwave_multiplier(temps):
        count_33 = count_35 = max_33 = max_35 = 0
        for t in temps:
            if t >= 33:
                count_33 += 1
                max_33 = max(max_33, count_33)
            else:
                count_33 = 0
            if t >= 35:
                count_35 += 1
                max_35 = max(max_35, count_35)
            else:
                count_35 = 0
        if max_35 >= 2:
            return 1.3
        elif max_33 >= 2:
            return 1.15
        else:
            return 1.0

    def calculate_damage_score_prescore(s, e, p_pred):
        return 100 * (0.25 * s + 0.25 * e + 0.5 * p_pred)

    def calculate_damage_score_final(s, e, p_pred, p_real):
        return 100 * (0.2 * s + 0.2 * e + 0.5 * p_pred + 0.1 * p_real)

    def score_to_grade(score):
        if score < 30: return "ë‚®ìŒ"
        elif score < 40: return "ë³´í†µ"
        elif score < 50: return "ë†’ìŒ"
        else: return "ë§¤ìš° ë†’ìŒ"

    def calc_payout(score):
        if score < 30: return 0
        elif score < 40: return 5000
        elif score < 50: return 10000
        else: return 20000

    def format_debug_log(row, date_str):
        return f"""[í”¼í•´ì ìˆ˜ ê³„ì‚° ë¡œê·¸ - {row['ìì¹˜êµ¬']} / {date_str}]
[S ê³„ì‚°] - ê³ ë ¹ìë¹„ìœ¨ = {row['ê³ ë ¹ìë¹„ìœ¨']:.4f}, ì•¼ì™¸ê·¼ë¡œìë¹„ìœ¨ = {row['ì•¼ì™¸ê·¼ë¡œìë¹„ìœ¨']:.4f}, ì—´ì·¨ì•½ì¸êµ¬ë¹„ìœ¨ = {row['ì—´ì¾Œì ì·¨ì•½ì¸êµ¬ë¹„ìœ¨']:.4f} â†’ S = {row['S']:.4f}
[E ê³„ì‚°] - ì—´ì„¬ì§€ìˆ˜ = {row['ì—´ì„¬ì§€ìˆ˜_std']:.4f}, ë…¹ì§€ìœ¨ = {row['ë…¹ì§€ìœ¨_std']:.4f}, ëƒ‰ë°©ë³´ê¸‰ë¥  = {row['ëƒ‰ë°©ë³´ê¸‰ë¥ _std']:.4f} â†’ E = {row['E']:.4f}
[P ê³„ì‚°] - ì˜ˆì¸¡í™˜ììˆ˜ = {row['P_pred_raw']:.2f}ëª… â†’ ì •ê·œí™”(P_pred) = {row['P_pred']:.4f}
[R ê³„ì‚°] - ì‹¤ì œí™˜ììˆ˜ = {row['í™˜ììˆ˜']}, ë³€í™˜(P_real) = {1.0 if row['í™˜ììˆ˜'] >= 1 else 0.0}
[H ê³„ì‚°] - í­ì—¼ê°€ì¤‘ì¹˜ = {row['H']:.2f}
ì‚¬ì „ì ìˆ˜ = {row['í”¼í•´ì ìˆ˜_ì‚¬ì „']:.2f} / ì‚¬í›„ì ìˆ˜ = {row['í”¼í•´ì ìˆ˜']:.2f} / ìœ„í—˜ë“±ê¸‰: {row['ìœ„í—˜ë“±ê¸‰']} / ë³´ìƒê¸ˆ: {row['ë³´ìƒê¸ˆ']}ì›
"""


    def calculate_social_index(row):
        return (
            0.5 * row["ê³ ë ¹ìë¹„ìœ¨"] +
            0.3 * row["ì•¼ì™¸ê·¼ë¡œìë¹„ìœ¨"] +
            0.2 * row["ì—´ì¾Œì ì·¨ì•½ì¸êµ¬ë¹„ìœ¨"]
        )

    def standardize_column(df, col):
        min_val = df[col].min()
        max_val = df[col].max()
        range_val = max_val - min_val if max_val != min_val else 1
        return (df[col] - min_val) / range_val

    def calculate_environment_index(row):
        return (
            0.5 * row["ì—´ì„¬ì§€ìˆ˜_std"] +
            0.3 * (1 - row["ë…¹ì§€ìœ¨_std"]) +
            0.2 * (1 - row["ëƒ‰ë°©ë³´ê¸‰ë¥ _std"])
        )

    def distribute_pred_by_s(merged_df, total_pred):
        s_sum = merged_df["S"].sum()
        merged_df["P_pred_raw"] = total_pred * (merged_df["S"] / s_sum)
        merged_df["P_pred"] = (merged_df["P_pred_raw"] / 25) ** 0.5
        return merged_df

    def load_csv_with_fallback(path):
        for enc in ["utf-8-sig", "cp949", "euc-kr"]:
            try:
                return pd.read_csv(path, encoding=enc)
            except UnicodeDecodeError:
                continue
        raise UnicodeDecodeError(f"ì¸ì½”ë”© ì‹¤íŒ¨: {path}")

    def load_csv_from_github(filename):
        try:
            github_url =   f"https://raw.githubusercontent.com/{GITHUB_USERNAME}/{GITHUB_REPO}/{GITHUB_BRANCH}/{filename}"
            r = requests.get(github_url)
            r.raise_for_status()
            return pd.read_csv(io.StringIO(r.text), encoding="utf-8-sig")
        except Exception as e:
            st.error(f"GitHubì—ì„œ {filename} ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()


    # ë©”ì¸ ì‹¤í–‰
    try:
        # ë‚ ì§œ ì„ íƒ (ë‹¨ì¼ ì¹¼ëŸ¼)
        today = datetime.date.today()
        min_date = today - datetime.timedelta(days=6)
        selected_date = st.date_input("ë¶„ì„ ê¸°ì¤€ì¼ ì„ íƒ (ìµœê·¼ 7ì¼)", today, min_value=min_date, max_value=today)
        ymd = selected_date.strftime("%Y-%m-%d")

        ml_data = load_csv_from_github("ML_asos_dataset.csv")
        if ml_data.empty:
            st.warning("ê¸°ë¡ëœ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. tab2ì—ì„œ ë°ì´í„°ë¥¼ ë¨¼ì € ì €ì¥í•´ì£¼ì„¸ìš”.")
            st.stop()
        ml_data = ml_data[ml_data["ì¼ì"] == ymd]
        static_data = load_csv_with_fallback("seoul_static_data.csv")
        merged_all = pd.merge(static_data, ml_data, on="ìì¹˜êµ¬", how="left")

        if merged_all.empty:
            st.warning("ì„ íƒí•œ ë‚ ì§œì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        df_total = load_csv_from_github("ML_asos_total_prediction.csv")
        pred_row = df_total[df_total["ì¼ì"] == ymd]

        if pred_row.empty:
            st.warning(f"{ymd} ì˜ˆì¸¡ê°’ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. tab1ì—ì„œ ë¨¼ì € ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
            st.stop()

        seoul_pred = float(pred_row["ì„œìš¸ì‹œì˜ˆì¸¡í™˜ììˆ˜"].values[0])

        merged_all["S"] = merged_all.apply(calculate_social_index, axis=1)
        for col in ["ì—´ì„¬ì§€ìˆ˜", "ë…¹ì§€ìœ¨", "ëƒ‰ë°©ë³´ê¸‰ë¥ "]:
            merged_all[f"{col}_std"] = standardize_column(merged_all, col)
        merged_all["E"] = merged_all.apply(calculate_environment_index, axis=1)

        merged_all = distribute_pred_by_s(merged_all, seoul_pred)

        merged_all["í”¼í•´ì ìˆ˜_ì‚¬ì „"] = merged_all.apply(
            lambda row: calculate_damage_score_prescore(
                row["S"], row["E"], row["P_pred"]
            ),
            axis=1
        )

        merged_all["í”¼í•´ì ìˆ˜"] = merged_all.apply(
            lambda row: calculate_damage_score_final(
                row["S"], row["E"], row["P_pred"], 1.0 if row["í™˜ììˆ˜"] >= 1 else 0.0
            ),
            axis=1
        )

        # í­ì—¼ ì§€ì†ì„± ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ë°˜ì˜
        heatwave_temps = merged_all.sort_values("ì¼ì").groupby("ìì¹˜êµ¬")["ìµœê³ ì²´ê°ì˜¨ë„(Â°C)"].apply(list)
        merged_all["H"] = merged_all["ìì¹˜êµ¬"].map(lambda gu: calculate_heatwave_multiplier(heatwave_temps.get(gu, [])))
        merged_all["í”¼í•´ì ìˆ˜_ì‚¬ì „"] *= merged_all["H"]
        merged_all["í”¼í•´ì ìˆ˜"] *= merged_all["H"]

        merged_all["ìœ„í—˜ë“±ê¸‰"] = merged_all["í”¼í•´ì ìˆ˜"].apply(score_to_grade)
        merged_all["ë³´ìƒê¸ˆ"] = merged_all["í”¼í•´ì ìˆ˜"].apply(calc_payout)

        col1, col2 = st.columns(2)
        with col1:
            selected_gu = st.selectbox("ìì¹˜êµ¬ ì„ íƒ", sorted(merged_all["ìì¹˜êµ¬"].unique()))
        with col2:
            subs_count = st.number_input(f"{selected_gu} ê°€ì…ì ìˆ˜", min_value=0, step=1, key="subs_tab3")

        merged = merged_all[merged_all["ìì¹˜êµ¬"] == selected_gu].copy()
        if merged.empty:
            st.warning("ì„ íƒí•œ ìì¹˜êµ¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        merged["ê°€ì…ììˆ˜"] = subs_count
        merged["ì˜ˆìƒì´ë³´ìƒê¸ˆ"] = merged["ë³´ìƒê¸ˆ"] * subs_count
        st.success(f"ì˜ˆìƒ ë³´ìƒê¸ˆì•¡: {int(merged['ì˜ˆìƒì´ë³´ìƒê¸ˆ'].sum()):,}ì›")

        show_cols = ["ìì¹˜êµ¬", "í”¼í•´ì ìˆ˜_ì‚¬ì „", "í”¼í•´ì ìˆ˜", "H", "ìœ„í—˜ë“±ê¸‰", "ë³´ìƒê¸ˆ", "ê°€ì…ììˆ˜", "ì˜ˆìƒì´ë³´ìƒê¸ˆ"]
        st.markdown("#### ìì¹˜êµ¬ë³„ í”¼í•´ì ìˆ˜ ë¹„êµ")
        st.dataframe(
            merged[show_cols],
            use_container_width=True
        )

        st.markdown("#### í”¼í•´ì ìˆ˜ ë¶„í¬ (ì‚¬í›„ ê¸°ì¤€)")
        st.bar_chart(data=merged_all.set_index("ìì¹˜êµ¬")["í”¼í•´ì ìˆ˜"])

        # ë‹¨ì¼ ìì¹˜êµ¬ ë””ë²„ê¹… ë¡œê·¸
        row = merged.iloc[0]
        single_log = format_debug_log(row, ymd)

        with st.expander(f"{selected_gu} ë””ë²„ê¹… ë¡œê·¸"):
            st.code(single_log, language="text")
            st.download_button(
                label="í˜„ì¬ ìì¹˜êµ¬ ë””ë²„ê¹… ë¡œê·¸ ë‹¤ìš´ë¡œë“œ",
                data=single_log.encode("utf-8-sig"),
                file_name=f"í”¼í•´ì ìˆ˜_ë””ë²„ê¹…_{ymd}_{selected_gu}.txt",
                mime="text/plain"
            )

        # ì „ì²´ ìì¹˜êµ¬ ë””ë²„ê¹… ë¡œê·¸
        all_debug_logs = "\n".join([
            format_debug_log(row, ymd) for _, row in merged_all.iterrows()
        ])
        st.download_button(
            label="ì „ì²´ ìì¹˜êµ¬ ë””ë²„ê¹… ë¡œê·¸ ë‹¤ìš´ë¡œë“œ",
            data=all_debug_logs.encode("utf-8-sig"),
            file_name=f"ì „ì²´_í”¼í•´ì ìˆ˜_ë””ë²„ê¹…_{ymd}.txt",
            mime="text/plain"
        )

    except Exception as e:
        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
